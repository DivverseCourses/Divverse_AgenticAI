{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated\n",
    "from typing_extensions import TypedDict\n",
    "from langgraph.graph.message import add_messages\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from langchain_core.tools import tool\n",
    "from langchain_experimental.utilities import PythonREPL\n",
    "from typing import Literal\n",
    "from langchain_core.messages import BaseMessage, HumanMessage\n",
    "from langchain_groq import ChatGroq\n",
    "from langgraph.graph import MessagesState, END, StateGraph, START\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "## Tools to use\n",
    "tavily_tool = TavilySearchResults(max_results=2)\n",
    "\n",
    "repl = PythonREPL()\n",
    "\n",
    "@tool\n",
    "def python_repl_tool(\n",
    "    code: Annotated[str, \"The python code to execute to generate your chart.\"],\n",
    "):\n",
    "    \"\"\"Use this to execute python code. If you want to see the output of a value,\n",
    "    you should print it out with `print(...)`. This is visible to the user.\"\"\"\n",
    "    try:\n",
    "        result = repl.run(code)\n",
    "    except BaseException as e:\n",
    "        return f\"Failed to execute. Error: {repr(e)}\"\n",
    "    result_str = f\"Successfully executed:\\n\\`\\`\\`python\\n{code}\\n\\`\\`\\`\\nStdout: {result}\"\n",
    "    return (\n",
    "        result_str + \"\\n\\nIf you have completed all tasks, respond with FINAL ANSWER.\"\n",
    "    )\n",
    "\n",
    "\n",
    "# Utility to create a system prompt for each agent\n",
    "def make_system_prompt(suffix: str) -> str:\n",
    "    return (\n",
    "        \"You are a helpful AI assistant, collaborating with other assistants.\"\n",
    "        \" Use the provided tools to progress towards answering the question.\"\n",
    "        \" If you are unable to fully answer, that's OK, another assistant with different tools \"\n",
    "        \" will help where you left off. Execute what you can to make progress.\"\n",
    "        \" If you or any of the other assistants have the final answer or deliverable,\"\n",
    "        \" prefix your response with FINAL ANSWER so the team knows to stop.\"\n",
    "        f\"\\n{suffix}\"\n",
    "    )\n",
    "\n",
    "llm = ChatGroq(\n",
    "    model=\"llama3-8b-8192\",\n",
    "    temperature=0.5,\n",
    "    max_tokens=None,\n",
    "    timeout=None,\n",
    "    max_retries=2,\n",
    ")\n",
    "\n",
    "llm_with_research_tool = llm.bind_tools([tavily_tool])\n",
    "llm_with_python_tool = llm.bind_tools([python_repl_tool])\n",
    "\n",
    "def route_logic(state: MessagesState):\n",
    "    \"\"\"Route logic to determine the next node.\"\"\"\n",
    "    print(\"Current state messages:\", state[\"messages\"])\n",
    "    if \"FINAL ANSWER\" in state[\"messages\"][-1].content:\n",
    "        return END\n",
    "    return \"chart_generator\" if \"chart\" in \\\n",
    "        state[\"messages\"][-1].content else \"researcher\"\n",
    "\n",
    "# Define the research node\n",
    "def research_node(state: MessagesState):\n",
    "    \"\"\"Node for conducting research using the Tavily search tool.\"\"\"\n",
    "    messages = state.get(\"messages\", [])\n",
    "    result = llm_with_research_tool.invoke(messages)\n",
    "    result[-1] = HumanMessage(content=result[-1].content, name=\"researcher\")\n",
    "    return {\n",
    "        \"messages\": result,\n",
    "    }\n",
    "\n",
    "# Define the chart generation node\n",
    "def chart_node(state: MessagesState):\n",
    "    \"\"\"Node for generating charts (placeholder functionality).\"\"\"\n",
    "    messages = state.get(\"messages\", [])\n",
    "    result = llm_with_python_tool.invoke(messages)\n",
    "    result[-1] = HumanMessage(content=result[-1].content, name=\"chart_generator\")\n",
    "    return {\n",
    "        \"messages\": result,\n",
    "    }\n",
    "\n",
    "\n",
    "# Define the state graph\n",
    "graph_builder = StateGraph(MessagesState)\n",
    "\n",
    "# Add nodes to the graph\n",
    "graph_builder.add_node(\"researcher\", research_node)\n",
    "graph_builder.add_node(\"chart_generator\", chart_node)\n",
    "\n",
    "# Add edges to the graph\n",
    "graph_builder.add_conditional_edges(\"researcher\", route_logic, {\"chart_generator\": \"chart_generator\", END: END})\n",
    "graph_builder.add_conditional_edges(\"chart_generator\", route_logic, {\"researcher\": \"researcher\", END: END})\n",
    "\n",
    "graph_builder.add_edge(START, \"researcher\")\n",
    "\n",
    "# Compile the graph\n",
    "graph = graph_builder.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "try:\n",
    "    display(Image(graph.get_graph().draw_mermaid_png()))\n",
    "except Exception:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "BadRequestError",
     "evalue": "Error code: 400 - {'error': {'message': \"Failed to call a function. Please adjust your prompt. See 'failed_generation' for more details.\", 'type': 'invalid_request_error', 'code': 'tool_use_failed', 'failed_generation': '<tool-use>\\n{\\n    \"tool_calls\": [\\n        {\\n            \"id\": \"pending\",\\n            \"type\": \"function\",\\n            \"function\": {\\n                \"name\": \"world_population_demographics\"\\n            },\\n            \"parameters\": {}\\n        }\\n    ]\\n}\\n</tool-use>'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mBadRequestError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m user_input \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mShow me a demographic of the world population\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m      3\u001b[0m state \u001b[39m=\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39mmessages\u001b[39m\u001b[39m\"\u001b[39m: [HumanMessage(content\u001b[39m=\u001b[39muser_input)]}\n\u001b[1;32m----> 4\u001b[0m \u001b[39mfor\u001b[39;49;00m event \u001b[39min\u001b[39;49;00m graph\u001b[39m.\u001b[39;49mstream(state):\n\u001b[0;32m      5\u001b[0m     \u001b[39mprint\u001b[39;49m(event)\n",
      "File \u001b[1;32mc:\\Users\\okwy_\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\langgraph\\pregel\\__init__.py:1336\u001b[0m, in \u001b[0;36mPregel.stream\u001b[1;34m(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, debug, subgraphs)\u001b[0m\n\u001b[0;32m   1325\u001b[0m     \u001b[39m# Similarly to Bulk Synchronous Parallel / Pregel model\u001b[39;00m\n\u001b[0;32m   1326\u001b[0m     \u001b[39m# computation proceeds in steps, while there are channel updates\u001b[39;00m\n\u001b[0;32m   1327\u001b[0m     \u001b[39m# channel updates from step N are only visible in step N+1\u001b[39;00m\n\u001b[0;32m   1328\u001b[0m     \u001b[39m# channels are guaranteed to be immutable for the duration of the step,\u001b[39;00m\n\u001b[0;32m   1329\u001b[0m     \u001b[39m# with channel updates applied only at the transition between steps\u001b[39;00m\n\u001b[0;32m   1330\u001b[0m     \u001b[39mwhile\u001b[39;00m loop\u001b[39m.\u001b[39mtick(\n\u001b[0;32m   1331\u001b[0m         input_keys\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39minput_channels,\n\u001b[0;32m   1332\u001b[0m         interrupt_before\u001b[39m=\u001b[39minterrupt_before_,\n\u001b[0;32m   1333\u001b[0m         interrupt_after\u001b[39m=\u001b[39minterrupt_after_,\n\u001b[0;32m   1334\u001b[0m         manager\u001b[39m=\u001b[39mrun_manager,\n\u001b[0;32m   1335\u001b[0m     ):\n\u001b[1;32m-> 1336\u001b[0m         \u001b[39mfor\u001b[39;49;00m _ \u001b[39min\u001b[39;49;00m runner\u001b[39m.\u001b[39;49mtick(\n\u001b[0;32m   1337\u001b[0m             loop\u001b[39m.\u001b[39;49mtasks\u001b[39m.\u001b[39;49mvalues(),\n\u001b[0;32m   1338\u001b[0m             timeout\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstep_timeout,\n\u001b[0;32m   1339\u001b[0m             retry_policy\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mretry_policy,\n\u001b[0;32m   1340\u001b[0m             get_waiter\u001b[39m=\u001b[39;49mget_waiter,\n\u001b[0;32m   1341\u001b[0m         ):\n\u001b[0;32m   1342\u001b[0m             \u001b[39m# emit output\u001b[39;49;00m\n\u001b[0;32m   1343\u001b[0m             \u001b[39myield from\u001b[39;49;00m output()\n\u001b[0;32m   1344\u001b[0m \u001b[39m# emit output\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\okwy_\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\langgraph\\pregel\\runner.py:58\u001b[0m, in \u001b[0;36mPregelRunner.tick\u001b[1;34m(self, tasks, reraise, timeout, retry_policy, get_waiter)\u001b[0m\n\u001b[0;32m     56\u001b[0m t \u001b[39m=\u001b[39m tasks[\u001b[39m0\u001b[39m]\n\u001b[0;32m     57\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 58\u001b[0m     run_with_retry(t, retry_policy)\n\u001b[0;32m     59\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommit(t, \u001b[39mNone\u001b[39;00m)\n\u001b[0;32m     60\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m exc:\n",
      "File \u001b[1;32mc:\\Users\\okwy_\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\langgraph\\pregel\\retry.py:29\u001b[0m, in \u001b[0;36mrun_with_retry\u001b[1;34m(task, retry_policy)\u001b[0m\n\u001b[0;32m     27\u001b[0m task\u001b[39m.\u001b[39mwrites\u001b[39m.\u001b[39mclear()\n\u001b[0;32m     28\u001b[0m \u001b[39m# run the task\u001b[39;00m\n\u001b[1;32m---> 29\u001b[0m task\u001b[39m.\u001b[39;49mproc\u001b[39m.\u001b[39;49minvoke(task\u001b[39m.\u001b[39;49minput, config)\n\u001b[0;32m     30\u001b[0m \u001b[39m# if successful, end\u001b[39;00m\n\u001b[0;32m     31\u001b[0m \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\okwy_\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\langgraph\\utils\\runnable.py:410\u001b[0m, in \u001b[0;36mRunnableSeq.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m    408\u001b[0m context\u001b[39m.\u001b[39mrun(_set_config_context, config)\n\u001b[0;32m    409\u001b[0m \u001b[39mif\u001b[39;00m i \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m--> 410\u001b[0m     \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m context\u001b[39m.\u001b[39;49mrun(step\u001b[39m.\u001b[39;49minvoke, \u001b[39minput\u001b[39;49m, config, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    411\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    412\u001b[0m     \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m context\u001b[39m.\u001b[39mrun(step\u001b[39m.\u001b[39minvoke, \u001b[39minput\u001b[39m, config)\n",
      "File \u001b[1;32mc:\\Users\\okwy_\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\langgraph\\utils\\runnable.py:184\u001b[0m, in \u001b[0;36mRunnableCallable.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m    182\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    183\u001b[0m     context\u001b[39m.\u001b[39mrun(_set_config_context, config)\n\u001b[1;32m--> 184\u001b[0m     ret \u001b[39m=\u001b[39m context\u001b[39m.\u001b[39;49mrun(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfunc, \u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    185\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(ret, Runnable) \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrecurse:\n\u001b[0;32m    186\u001b[0m     \u001b[39mreturn\u001b[39;00m ret\u001b[39m.\u001b[39minvoke(\u001b[39minput\u001b[39m, config)\n",
      "Cell \u001b[1;32mIn[1], line 71\u001b[0m, in \u001b[0;36mresearch_node\u001b[1;34m(state)\u001b[0m\n\u001b[0;32m     69\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Node for conducting research using the Tavily search tool.\"\"\"\u001b[39;00m\n\u001b[0;32m     70\u001b[0m messages \u001b[39m=\u001b[39m state\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mmessages\u001b[39m\u001b[39m\"\u001b[39m, [])\n\u001b[1;32m---> 71\u001b[0m result \u001b[39m=\u001b[39m llm_with_research_tool\u001b[39m.\u001b[39;49minvoke(messages)\n\u001b[0;32m     72\u001b[0m result[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m] \u001b[39m=\u001b[39m HumanMessage(content\u001b[39m=\u001b[39mresult[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39mcontent, name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mresearcher\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     73\u001b[0m \u001b[39mreturn\u001b[39;00m {\n\u001b[0;32m     74\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mmessages\u001b[39m\u001b[39m\"\u001b[39m: result,\n\u001b[0;32m     75\u001b[0m }\n",
      "File \u001b[1;32mc:\\Users\\okwy_\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\langchain_core\\runnables\\base.py:5354\u001b[0m, in \u001b[0;36mRunnableBindingBase.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m   5348\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39minvoke\u001b[39m(\n\u001b[0;32m   5349\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m   5350\u001b[0m     \u001b[39minput\u001b[39m: Input,\n\u001b[0;32m   5351\u001b[0m     config: Optional[RunnableConfig] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m   5352\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Optional[Any],\n\u001b[0;32m   5353\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Output:\n\u001b[1;32m-> 5354\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbound\u001b[39m.\u001b[39;49minvoke(\n\u001b[0;32m   5355\u001b[0m         \u001b[39minput\u001b[39;49m,\n\u001b[0;32m   5356\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_merge_configs(config),\n\u001b[0;32m   5357\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m{\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mkwargs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs},\n\u001b[0;32m   5358\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\okwy_\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:286\u001b[0m, in \u001b[0;36mBaseChatModel.invoke\u001b[1;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[0;32m    275\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39minvoke\u001b[39m(\n\u001b[0;32m    276\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    277\u001b[0m     \u001b[39minput\u001b[39m: LanguageModelInput,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    281\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any,\n\u001b[0;32m    282\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m BaseMessage:\n\u001b[0;32m    283\u001b[0m     config \u001b[39m=\u001b[39m ensure_config(config)\n\u001b[0;32m    284\u001b[0m     \u001b[39mreturn\u001b[39;00m cast(\n\u001b[0;32m    285\u001b[0m         ChatGeneration,\n\u001b[1;32m--> 286\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgenerate_prompt(\n\u001b[0;32m    287\u001b[0m             [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_convert_input(\u001b[39minput\u001b[39;49m)],\n\u001b[0;32m    288\u001b[0m             stop\u001b[39m=\u001b[39;49mstop,\n\u001b[0;32m    289\u001b[0m             callbacks\u001b[39m=\u001b[39;49mconfig\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mcallbacks\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[0;32m    290\u001b[0m             tags\u001b[39m=\u001b[39;49mconfig\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mtags\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[0;32m    291\u001b[0m             metadata\u001b[39m=\u001b[39;49mconfig\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mmetadata\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[0;32m    292\u001b[0m             run_name\u001b[39m=\u001b[39;49mconfig\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mrun_name\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[0;32m    293\u001b[0m             run_id\u001b[39m=\u001b[39;49mconfig\u001b[39m.\u001b[39;49mpop(\u001b[39m\"\u001b[39;49m\u001b[39mrun_id\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m    294\u001b[0m             \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[0;32m    295\u001b[0m         )\u001b[39m.\u001b[39mgenerations[\u001b[39m0\u001b[39m][\u001b[39m0\u001b[39m],\n\u001b[0;32m    296\u001b[0m     )\u001b[39m.\u001b[39mmessage\n",
      "File \u001b[1;32mc:\\Users\\okwy_\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:786\u001b[0m, in \u001b[0;36mBaseChatModel.generate_prompt\u001b[1;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m    778\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mgenerate_prompt\u001b[39m(\n\u001b[0;32m    779\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    780\u001b[0m     prompts: \u001b[39mlist\u001b[39m[PromptValue],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    783\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any,\n\u001b[0;32m    784\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m LLMResult:\n\u001b[0;32m    785\u001b[0m     prompt_messages \u001b[39m=\u001b[39m [p\u001b[39m.\u001b[39mto_messages() \u001b[39mfor\u001b[39;00m p \u001b[39min\u001b[39;00m prompts]\n\u001b[1;32m--> 786\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgenerate(prompt_messages, stop\u001b[39m=\u001b[39;49mstop, callbacks\u001b[39m=\u001b[39;49mcallbacks, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\okwy_\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:643\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[1;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[0;32m    641\u001b[0m         \u001b[39mif\u001b[39;00m run_managers:\n\u001b[0;32m    642\u001b[0m             run_managers[i]\u001b[39m.\u001b[39mon_llm_error(e, response\u001b[39m=\u001b[39mLLMResult(generations\u001b[39m=\u001b[39m[]))\n\u001b[1;32m--> 643\u001b[0m         \u001b[39mraise\u001b[39;00m e\n\u001b[0;32m    644\u001b[0m flattened_outputs \u001b[39m=\u001b[39m [\n\u001b[0;32m    645\u001b[0m     LLMResult(generations\u001b[39m=\u001b[39m[res\u001b[39m.\u001b[39mgenerations], llm_output\u001b[39m=\u001b[39mres\u001b[39m.\u001b[39mllm_output)  \u001b[39m# type: ignore[list-item]\u001b[39;00m\n\u001b[0;32m    646\u001b[0m     \u001b[39mfor\u001b[39;00m res \u001b[39min\u001b[39;00m results\n\u001b[0;32m    647\u001b[0m ]\n\u001b[0;32m    648\u001b[0m llm_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_combine_llm_outputs([res\u001b[39m.\u001b[39mllm_output \u001b[39mfor\u001b[39;00m res \u001b[39min\u001b[39;00m results])\n",
      "File \u001b[1;32mc:\\Users\\okwy_\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:633\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[1;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[0;32m    630\u001b[0m \u001b[39mfor\u001b[39;00m i, m \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(messages):\n\u001b[0;32m    631\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    632\u001b[0m         results\u001b[39m.\u001b[39mappend(\n\u001b[1;32m--> 633\u001b[0m             \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_generate_with_cache(\n\u001b[0;32m    634\u001b[0m                 m,\n\u001b[0;32m    635\u001b[0m                 stop\u001b[39m=\u001b[39;49mstop,\n\u001b[0;32m    636\u001b[0m                 run_manager\u001b[39m=\u001b[39;49mrun_managers[i] \u001b[39mif\u001b[39;49;00m run_managers \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[0;32m    637\u001b[0m                 \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[0;32m    638\u001b[0m             )\n\u001b[0;32m    639\u001b[0m         )\n\u001b[0;32m    640\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    641\u001b[0m         \u001b[39mif\u001b[39;00m run_managers:\n",
      "File \u001b[1;32mc:\\Users\\okwy_\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:851\u001b[0m, in \u001b[0;36mBaseChatModel._generate_with_cache\u001b[1;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m    849\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    850\u001b[0m     \u001b[39mif\u001b[39;00m inspect\u001b[39m.\u001b[39msignature(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_generate)\u001b[39m.\u001b[39mparameters\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mrun_manager\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m--> 851\u001b[0m         result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_generate(\n\u001b[0;32m    852\u001b[0m             messages, stop\u001b[39m=\u001b[39;49mstop, run_manager\u001b[39m=\u001b[39;49mrun_manager, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs\n\u001b[0;32m    853\u001b[0m         )\n\u001b[0;32m    854\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    855\u001b[0m         result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_generate(messages, stop\u001b[39m=\u001b[39mstop, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\okwy_\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\langchain_groq\\chat_models.py:474\u001b[0m, in \u001b[0;36mChatGroq._generate\u001b[1;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m    469\u001b[0m message_dicts, params \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_create_message_dicts(messages, stop)\n\u001b[0;32m    470\u001b[0m params \u001b[39m=\u001b[39m {\n\u001b[0;32m    471\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams,\n\u001b[0;32m    472\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m    473\u001b[0m }\n\u001b[1;32m--> 474\u001b[0m response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mclient\u001b[39m.\u001b[39;49mcreate(messages\u001b[39m=\u001b[39;49mmessage_dicts, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mparams)\n\u001b[0;32m    475\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_create_chat_result(response)\n",
      "File \u001b[1;32mc:\\Users\\okwy_\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\groq\\resources\\chat\\completions.py:178\u001b[0m, in \u001b[0;36mCompletions.create\u001b[1;34m(self, messages, model, frequency_penalty, logit_bias, logprobs, max_tokens, n, presence_penalty, response_format, seed, stop, stream, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[0;32m    135\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcreate\u001b[39m(\n\u001b[0;32m    136\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    137\u001b[0m     \u001b[39m*\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    161\u001b[0m     timeout: \u001b[39mfloat\u001b[39m \u001b[39m|\u001b[39m httpx\u001b[39m.\u001b[39mTimeout \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m|\u001b[39m NotGiven \u001b[39m=\u001b[39m NOT_GIVEN,\n\u001b[0;32m    162\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m ChatCompletion \u001b[39m|\u001b[39m Stream[ChatCompletionChunk]:\n\u001b[0;32m    163\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    164\u001b[0m \u001b[39m    Creates a completion for a chat prompt\u001b[39;00m\n\u001b[0;32m    165\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    176\u001b[0m \u001b[39m      timeout: Override the client-level default timeout for this request, in seconds\u001b[39;00m\n\u001b[0;32m    177\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 178\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_post(\n\u001b[0;32m    179\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39m/openai/v1/chat/completions\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    180\u001b[0m         body\u001b[39m=\u001b[39;49mmaybe_transform(\n\u001b[0;32m    181\u001b[0m             {\n\u001b[0;32m    182\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mmessages\u001b[39;49m\u001b[39m\"\u001b[39;49m: messages,\n\u001b[0;32m    183\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mmodel\u001b[39;49m\u001b[39m\"\u001b[39;49m: model,\n\u001b[0;32m    184\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mfrequency_penalty\u001b[39;49m\u001b[39m\"\u001b[39;49m: frequency_penalty,\n\u001b[0;32m    185\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mlogit_bias\u001b[39;49m\u001b[39m\"\u001b[39;49m: logit_bias,\n\u001b[0;32m    186\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mlogprobs\u001b[39;49m\u001b[39m\"\u001b[39;49m: logprobs,\n\u001b[0;32m    187\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mmax_tokens\u001b[39;49m\u001b[39m\"\u001b[39;49m: max_tokens,\n\u001b[0;32m    188\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mn\u001b[39;49m\u001b[39m\"\u001b[39;49m: n,\n\u001b[0;32m    189\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mpresence_penalty\u001b[39;49m\u001b[39m\"\u001b[39;49m: presence_penalty,\n\u001b[0;32m    190\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mresponse_format\u001b[39;49m\u001b[39m\"\u001b[39;49m: response_format,\n\u001b[0;32m    191\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mseed\u001b[39;49m\u001b[39m\"\u001b[39;49m: seed,\n\u001b[0;32m    192\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mstop\u001b[39;49m\u001b[39m\"\u001b[39;49m: stop,\n\u001b[0;32m    193\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mstream\u001b[39;49m\u001b[39m\"\u001b[39;49m: stream,\n\u001b[0;32m    194\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mtemperature\u001b[39;49m\u001b[39m\"\u001b[39;49m: temperature,\n\u001b[0;32m    195\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mtool_choice\u001b[39;49m\u001b[39m\"\u001b[39;49m: tool_choice,\n\u001b[0;32m    196\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mtools\u001b[39;49m\u001b[39m\"\u001b[39;49m: tools,\n\u001b[0;32m    197\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mtop_logprobs\u001b[39;49m\u001b[39m\"\u001b[39;49m: top_logprobs,\n\u001b[0;32m    198\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mtop_p\u001b[39;49m\u001b[39m\"\u001b[39;49m: top_p,\n\u001b[0;32m    199\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39muser\u001b[39;49m\u001b[39m\"\u001b[39;49m: user,\n\u001b[0;32m    200\u001b[0m             },\n\u001b[0;32m    201\u001b[0m             completion_create_params\u001b[39m.\u001b[39;49mCompletionCreateParams,\n\u001b[0;32m    202\u001b[0m         ),\n\u001b[0;32m    203\u001b[0m         options\u001b[39m=\u001b[39;49mmake_request_options(\n\u001b[0;32m    204\u001b[0m             extra_headers\u001b[39m=\u001b[39;49mextra_headers, extra_query\u001b[39m=\u001b[39;49mextra_query, extra_body\u001b[39m=\u001b[39;49mextra_body, timeout\u001b[39m=\u001b[39;49mtimeout\n\u001b[0;32m    205\u001b[0m         ),\n\u001b[0;32m    206\u001b[0m         cast_to\u001b[39m=\u001b[39;49mChatCompletion,\n\u001b[0;32m    207\u001b[0m         stream\u001b[39m=\u001b[39;49mstream \u001b[39mor\u001b[39;49;00m \u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m    208\u001b[0m         stream_cls\u001b[39m=\u001b[39;49mStream[ChatCompletionChunk],\n\u001b[0;32m    209\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\okwy_\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\groq\\_base_client.py:1194\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[1;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1180\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpost\u001b[39m(\n\u001b[0;32m   1181\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m   1182\u001b[0m     path: \u001b[39mstr\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1189\u001b[0m     stream_cls: \u001b[39mtype\u001b[39m[_StreamT] \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m   1190\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m ResponseT \u001b[39m|\u001b[39m _StreamT:\n\u001b[0;32m   1191\u001b[0m     opts \u001b[39m=\u001b[39m FinalRequestOptions\u001b[39m.\u001b[39mconstruct(\n\u001b[0;32m   1192\u001b[0m         method\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mpost\u001b[39m\u001b[39m\"\u001b[39m, url\u001b[39m=\u001b[39mpath, json_data\u001b[39m=\u001b[39mbody, files\u001b[39m=\u001b[39mto_httpx_files(files), \u001b[39m*\u001b[39m\u001b[39m*\u001b[39moptions\n\u001b[0;32m   1193\u001b[0m     )\n\u001b[1;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m cast(ResponseT, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrequest(cast_to, opts, stream\u001b[39m=\u001b[39;49mstream, stream_cls\u001b[39m=\u001b[39;49mstream_cls))\n",
      "File \u001b[1;32mc:\\Users\\okwy_\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\groq\\_base_client.py:896\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[1;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[0;32m    887\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrequest\u001b[39m(\n\u001b[0;32m    888\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    889\u001b[0m     cast_to: Type[ResponseT],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    894\u001b[0m     stream_cls: \u001b[39mtype\u001b[39m[_StreamT] \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m    895\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m ResponseT \u001b[39m|\u001b[39m _StreamT:\n\u001b[1;32m--> 896\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_request(\n\u001b[0;32m    897\u001b[0m         cast_to\u001b[39m=\u001b[39;49mcast_to,\n\u001b[0;32m    898\u001b[0m         options\u001b[39m=\u001b[39;49moptions,\n\u001b[0;32m    899\u001b[0m         stream\u001b[39m=\u001b[39;49mstream,\n\u001b[0;32m    900\u001b[0m         stream_cls\u001b[39m=\u001b[39;49mstream_cls,\n\u001b[0;32m    901\u001b[0m         remaining_retries\u001b[39m=\u001b[39;49mremaining_retries,\n\u001b[0;32m    902\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\okwy_\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\groq\\_base_client.py:987\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[1;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[0;32m    984\u001b[0m         err\u001b[39m.\u001b[39mresponse\u001b[39m.\u001b[39mread()\n\u001b[0;32m    986\u001b[0m     log\u001b[39m.\u001b[39mdebug(\u001b[39m\"\u001b[39m\u001b[39mRe-raising status error\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 987\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_make_status_error_from_response(err\u001b[39m.\u001b[39mresponse) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    989\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_process_response(\n\u001b[0;32m    990\u001b[0m     cast_to\u001b[39m=\u001b[39mcast_to,\n\u001b[0;32m    991\u001b[0m     options\u001b[39m=\u001b[39moptions,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    994\u001b[0m     stream_cls\u001b[39m=\u001b[39mstream_cls,\n\u001b[0;32m    995\u001b[0m )\n",
      "\u001b[1;31mBadRequestError\u001b[0m: Error code: 400 - {'error': {'message': \"Failed to call a function. Please adjust your prompt. See 'failed_generation' for more details.\", 'type': 'invalid_request_error', 'code': 'tool_use_failed', 'failed_generation': '<tool-use>\\n{\\n    \"tool_calls\": [\\n        {\\n            \"id\": \"pending\",\\n            \"type\": \"function\",\\n            \"function\": {\\n                \"name\": \"world_population_demographics\"\\n            },\\n            \"parameters\": {}\\n        }\\n    ]\\n}\\n</tool-use>'}}"
     ]
    }
   ],
   "source": [
    "user_input = 'Show me a demographic of the world population'\n",
    "\n",
    "state = {\"messages\": [HumanMessage(content=user_input)]}\n",
    "for event in graph.stream(state):\n",
    "    print(event)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e7421b2e8129b806acc53724fe4148ce7796e798f19ab33b4a2af02b8d0aa638"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
